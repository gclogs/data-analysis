{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ab606dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3 konlpy==0.6.0 gensim==4.3.2 JPype1==1.4.1 numpy pandas scikit-learn matplotlib lxml>=4.9.0 beautifulsoup4>=4.11.0 requests>=2.28.0 argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3b504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall JPype1 konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30c9de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석기 초기화 완료\n",
      "KNU 감성 사전 로드 중...\n",
      "감성 사전 로드 완료: 14841개 단어\n",
      "저장된 모델 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train] [--crawl]\n",
      "                             [--keywords KEYWORDS [KEYWORDS ...]]\n",
      "                             [--pages PAGES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\woig9\\AppData\\Roaming\\jupyter\\runtime\\kernel-v39c3ef3451f401fcd5d0758316da778b16e692b66.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 410\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m저장된 모델 로드 중...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     sentiment_model, vectorizer, policy_model = \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_okt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# 모델 로드 실패 시 새 모델 학습\u001b[39;00m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sentiment_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m vectorizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mload_models\u001b[39m\u001b[34m(model_dir, use_okt)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[33;03m저장된 모델 파일 로드\u001b[39;00m\n\u001b[32m    247\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03mtuple: (sentiment_model, tfidf_vectorizer, policy_model) 튜플\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     model_dir = os.path.join(os.path.dirname(os.path.abspath(\u001b[34;43m__file__\u001b[39;49m)), \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    258\u001b[39m sentiment_model = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    259\u001b[39m tfidf_vectorizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# SentiWord_info.json 데이터를 활용한 학습 데이터 생성\n",
    "def create_training_data_from_sentiword(sentiment_dict, sample_size=1000):\n",
    "    \"\"\"\n",
    "    KNU 감성 사전에서 학습 데이터 생성\n",
    "    \n",
    "    Parameters:\n",
    "    sentiment_dict (dict): 단어-감성 점수 딕셔너리\n",
    "    sample_size (int): 생성할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 학습용 데이터프레임\n",
    "    \"\"\"\n",
    "    print(f\"감성 사전에서 학습 데이터 생성 중 (목표 샘플 수: {sample_size})...\")\n",
    "    \n",
    "    # 긍정/부정 단어 분류\n",
    "    positive_words = [word for word, score in sentiment_dict.items() if score > 0]\n",
    "    negative_words = [word for word, score in sentiment_dict.items() if score < 0]\n",
    "    \n",
    "    # 긍정/부정 단어 수 확인\n",
    "    print(f\"긍정 단어 수: {len(positive_words)}, 부정 단어 수: {len(negative_words)}\")\n",
    "    \n",
    "    # 각 범주별 목표 샘플 수 계산\n",
    "    target_per_category = sample_size // 2\n",
    "    \n",
    "    # 단어 샘플링 (범주별 목표 샘플 수 만큼, 중복 허용)\n",
    "    positive_samples = random.choices(positive_words, k=target_per_category)\n",
    "    negative_samples = random.choices(negative_words, k=target_per_category)\n",
    "    \n",
    "    # 학습 데이터 생성\n",
    "    training_data = []\n",
    "    \n",
    "    # 긍정 샘플\n",
    "    for word in positive_samples:\n",
    "        training_data.append({\n",
    "            'document': word,\n",
    "            'label': 1  # 긍정\n",
    "        })\n",
    "    \n",
    "    # 부정 샘플\n",
    "    for word in negative_samples:\n",
    "        training_data.append({\n",
    "            'document': word,\n",
    "            'label': 0  # 부정\n",
    "        })\n",
    "    \n",
    "    # 데이터프레임 생성 및 섞기\n",
    "    df = pd.DataFrame(training_data)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # 데이터 섞기\n",
    "    \n",
    "    print(f\"학습 데이터 생성 완료: {len(df)}개 샘플\")\n",
    "    return df\n",
    "\n",
    "# 웹 크롤링을 통한 뉴스 및 정치 관련 데이터 수집 함수\n",
    "def crawl_news_data(keywords, num_pages=1, save_path=None):\n",
    "    \"\"\"\n",
    "    requests와 BeautifulSoup을 사용한 뉴스 크롤링 함수\n",
    "    \n",
    "    Parameters:\n",
    "    keywords (list): 검색 키워드 리스트\n",
    "    num_pages (int): 수집할 페이지 수\n",
    "    save_path (str): 결과를 저장할 경로\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 수집된 뉴스 데이터\n",
    "    \"\"\"\n",
    "    news_data = []\n",
    "    \n",
    "    print(f\"{', '.join(keywords)} 관련 뉴스 데이터 수집 중...\")\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        for page in range(1, num_pages + 1):\n",
    "            try:\n",
    "                # 네이버 뉴스 검색 URL (예시)\n",
    "                url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&start={(page-1)*10+1}\"\n",
    "                \n",
    "                # 요청 헤더 설정 (차단 방지)\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                }\n",
    "                \n",
    "                # 요청 보내기\n",
    "                response = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 뉴스 아이템 추출 (예시, 실제 선택자는 웹사이트에 따라 다름)\n",
    "                news_items = soup.select('.news_wrap')\n",
    "                \n",
    "                for item in news_items:\n",
    "                    try:\n",
    "                        # 제목과 요약 추출 (예시, 실제 선택자는 웹사이트에 따라 다름)\n",
    "                        title_elem = item.select_one('.news_tit')\n",
    "                        summary_elem = item.select_one('.dsc_wrap')\n",
    "                        \n",
    "                        if title_elem and summary_elem:\n",
    "                            title = title_elem.text.strip()\n",
    "                            summary = summary_elem.text.strip()\n",
    "                            \n",
    "                            news_data.append({\n",
    "                                'keyword': keyword,\n",
    "                                'title': title,\n",
    "                                'summary': summary,\n",
    "                                'content': f\"{title} {summary}\",\n",
    "                                'label': None  # 라벨은 나중에 수동으로 추가하거나 모델로 예측\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"뉴스 아이템 파싱 오류: {e}\")\n",
    "                \n",
    "                # 과도한 요청 방지를 위한 대기\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"페이지 크롤링 오류: {e}\")\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    \n",
    "    # 결과 저장\n",
    "    if save_path and len(news_data) > 0:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        news_df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "        print(f\"수집 데이터 저장 완료: {save_path}\")\n",
    "    \n",
    "    print(f\"뉴스 데이터 수집 완료: {len(news_data)}개 항목\")\n",
    "    return news_df\n",
    "\n",
    "# 메인 함수: 감성 사전 기반 및 ML 모델 결합\n",
    "def train_sentiment_model(use_speech_data=True, use_sentiword_data=True, policy_area_classification=True, save_model=True):\n",
    "    \"\"\"\n",
    "    감성 분석 모델 학습\n",
    "    \n",
    "    Parameters:\n",
    "    use_speech_data (bool): 연설문 데이터 사용 여부\n",
    "    use_sentiword_data (bool): 감성 사전 데이터 사용 여부\n",
    "    policy_area_classification (bool): 정책 분야 분류 모델 학습 여부\n",
    "    save_model (bool): 모델 저장 여부\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (감성 모델, 벡터라이저, OKT 객체, 정책 모델, 감성 사전)\n",
    "    \"\"\"\n",
    "    # KNU 감성 사전 로드\n",
    "    sentiment_dict = load_knu_sentiment_dict()\n",
    "    \n",
    "    # 형태소 분석기 초기화\n",
    "    okt = Okt()\n",
    "    print(\"OKT 형태소 분석기 초기화 완료\")\n",
    "    \n",
    "    # 학습 데이터 준비\n",
    "    training_data = pd.DataFrame()\n",
    "    \n",
    "    # 1. 연설문 데이터 로드 (선택적)\n",
    "    if use_speech_data:\n",
    "        speech_data = load_or_create_speech_data()\n",
    "        print(f\"연설문 데이터 로드 완료 (샘플 수: {len(speech_data)}개)\")\n",
    "        training_data = pd.concat([training_data, speech_data])\n",
    "    \n",
    "    # 2. SentiWord_info.json 데이터에서 학습 데이터 생성 (선택적)\n",
    "    if use_sentiword_data and sentiment_dict:\n",
    "        sentiword_data = create_training_data_from_sentiword(sentiment_dict, sample_size=2000)\n",
    "        if not sentiword_data.empty:\n",
    "            # 'policy_area' 열이 없으면 추가\n",
    "            if 'policy_area' not in sentiword_data.columns:\n",
    "                sentiword_data['policy_area'] = '기타'\n",
    "            training_data = pd.concat([training_data, sentiword_data])\n",
    "    \n",
    "    # 학습 데이터 전처리 및 모델 학습\n",
    "    sentiment_model = None\n",
    "    policy_model = None\n",
    "    tfidf_vectorizer = None\n",
    "    \n",
    "    if not training_data.empty:\n",
    "        print(f\"전체 학습 데이터 크기: {len(training_data)}개 샘플\")\n",
    "        print(\"텍스트 전처리 중...\")\n",
    "        \n",
    "        # 'document' 열의 널값 확인 및 처리\n",
    "        training_data = training_data.dropna(subset=['document'])\n",
    "        \n",
    "        # 텍스트 전처리\n",
    "        training_data['processed'] = training_data['document'].apply(\n",
    "            lambda x: preprocess_text(x, okt, is_political='policy_area' in training_data.columns)\n",
    "        )\n",
    "        \n",
    "        # 텍스트 벡터화\n",
    "        print(\"TF-IDF 벡터화 중...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "        train_tfidf = tfidf_vectorizer.fit_transform(training_data['processed'])\n",
    "        \n",
    "        # 감성 분석 모델 (RandomForest)\n",
    "        print(\"감성 분석 모델 학습 중...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            train_tfidf, training_data['label'], test_size=0.2, random_state=42\n",
    "        )\n",
    "        sentiment_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        sentiment_model.fit(X_train, y_train)\n",
    "        \n",
    "        # 모델 평가\n",
    "        y_pred = sentiment_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"감성 분석 모델 정확도: {accuracy:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # 정책 분야 분류 모델 (선택적)\n",
    "        if policy_area_classification and 'policy_area' in training_data.columns:\n",
    "            print(\"정책 분야 분류 모델 학습 중...\")\n",
    "            X_policy_train, X_policy_test, y_policy_train, y_policy_test = train_test_split(\n",
    "                train_tfidf, training_data['policy_area'], test_size=0.2, random_state=42\n",
    "            )\n",
    "            policy_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            policy_model.fit(X_policy_train, y_policy_train)\n",
    "            \n",
    "            # 모델 평가\n",
    "            y_policy_pred = policy_model.predict(X_policy_test)\n",
    "            policy_accuracy = accuracy_score(y_policy_test, y_policy_pred)\n",
    "            print(f\"정책 분야 분류 모델 정확도: {policy_accuracy:.4f}\")\n",
    "        \n",
    "        # 모델 저장 (선택적)\n",
    "        if save_model:\n",
    "            model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"models\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # 감성 분석 모델 저장\n",
    "            with open(os.path.join(model_dir, \"sentiment_model_okt.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(sentiment_model, f)\n",
    "            \n",
    "            # 정책 분야 분류 모델 저장 (있는 경우)\n",
    "            if policy_model is not None:\n",
    "                with open(os.path.join(model_dir, \"policy_model_okt.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(policy_model, f)\n",
    "            \n",
    "            # TF-IDF 벡터라이저 저장\n",
    "            with open(os.path.join(model_dir, \"tfidf_vectorizer_okt.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(tfidf_vectorizer, f)\n",
    "            \n",
    "            print(f\"모델 저장 완료: {model_dir}\")\n",
    "    else:\n",
    "        # 학습 데이터가 없을 경우 더미 모델 생성\n",
    "        print(\"경고: 학습 데이터가 없어 더미 모델을 생성합니다.\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "        dummy_texts = [\"긍정 텍스트\", \"부정 텍스트\"]\n",
    "        tfidf_vectorizer.fit(dummy_texts)\n",
    "    \n",
    "    return sentiment_model, tfidf_vectorizer, okt, policy_model, sentiment_dict\n",
    "\n",
    "# 모델 로드 함수\n",
    "def load_models(model_dir=None, use_okt=True):\n",
    "    \"\"\"\n",
    "    저장된 모델 파일 로드\n",
    "    \n",
    "    Parameters:\n",
    "    model_dir (str): 모델 파일이 저장된 디렉토리 경로\n",
    "    use_okt (bool): OKT 모델 사용 여부 (True: OKT, False: MeCab)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (sentiment_model, tfidf_vectorizer, policy_model) 튜플\n",
    "    \"\"\"\n",
    "    if model_dir is None:\n",
    "        model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"models\")\n",
    "    \n",
    "    sentiment_model = None\n",
    "    tfidf_vectorizer = None\n",
    "    policy_model = None\n",
    "    \n",
    "    try:\n",
    "        # 파일명 설정 (OKT 또는 MeCab)\n",
    "        suffix = \"_okt\" if use_okt else \"\"\n",
    "        \n",
    "        # 감성 분석 모델 로드\n",
    "        sentiment_model_path = os.path.join(model_dir, f\"sentiment_model{suffix}.pkl\")\n",
    "        if os.path.exists(sentiment_model_path):\n",
    "            with open(sentiment_model_path, \"rb\") as f:\n",
    "                sentiment_model = pickle.load(f)\n",
    "            print(\"감성 분석 모델 로드 완료\")\n",
    "        \n",
    "        # TF-IDF 벡터라이저 로드\n",
    "        vectorizer_path = os.path.join(model_dir, f\"tfidf_vectorizer{suffix}.pkl\")\n",
    "        if os.path.exists(vectorizer_path):\n",
    "            with open(vectorizer_path, \"rb\") as f:\n",
    "                tfidf_vectorizer = pickle.load(f)\n",
    "            print(\"TF-IDF 벡터라이저 로드 완료\")\n",
    "        \n",
    "        # 정책 분야 분류 모델 로드\n",
    "        policy_model_path = os.path.join(model_dir, f\"policy_model{suffix}.pkl\")\n",
    "        if os.path.exists(policy_model_path):\n",
    "            with open(policy_model_path, \"rb\") as f:\n",
    "                policy_model = pickle.load(f)\n",
    "            print(\"정책 분야 분류 모델 로드 완료\")\n",
    "        \n",
    "        return sentiment_model, tfidf_vectorizer, policy_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 오류: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 새로운 텍스트에 대한 감성 분석 및 정책 분야 분류 함수\n",
    "def analyze_text(text, sentiment_model, tfidf_vectorizer, okt, policy_model, sentiment_dict):\n",
    "    \"\"\"\n",
    "    텍스트에 대한 종합 분석 수행: \n",
    "    1. 감성 사전 기반 감성 분석\n",
    "    2. ML 모델 기반 감성 분석 (모델이 있는 경우)\n",
    "    3. 정책 분야 분류 (모델이 있는 경우 모델 사용, 없으면 규칙 기반)\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 분석할 텍스트\n",
    "    sentiment_model: 감성 분석 모델\n",
    "    tfidf_vectorizer: TF-IDF 벡터라이저\n",
    "    okt (Okt): OKT 형태소 분석기 객체\n",
    "    policy_model: 정책 분야 분류 모델\n",
    "    sentiment_dict (dict): 감성 사전\n",
    "    \n",
    "    Returns:\n",
    "    dict: 분석 결과\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 텍스트 전처리\n",
    "    processed_text = preprocess_text(text, okt, is_political=True)\n",
    "    \n",
    "    # 1. 감성 사전 기반 감성 분석\n",
    "    lexicon_sentiment, lexicon_score = analyze_sentiment_with_lexicon(text, okt, sentiment_dict)\n",
    "    results['lexicon_sentiment'] = {\n",
    "        'label': lexicon_sentiment,\n",
    "        'score': lexicon_score\n",
    "    }\n",
    "    \n",
    "    # 2. ML 모델 기반 감성 분석 (모델이 있는 경우)\n",
    "    if sentiment_model is not None and tfidf_vectorizer is not None:\n",
    "        # TF-IDF 벡터화\n",
    "        text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
    "        \n",
    "        # 감성 예측\n",
    "        sentiment = sentiment_model.predict(text_tfidf)[0]\n",
    "        probability = sentiment_model.predict_proba(text_tfidf)[0]\n",
    "        \n",
    "        sentiment_label = \"긍정\" if sentiment == 1 else \"부정\"\n",
    "        confidence = probability[1] if sentiment == 1 else probability[0]\n",
    "        \n",
    "        results['model_sentiment'] = {\n",
    "            'label': sentiment_label,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "    \n",
    "    # 3. 정책 분야 분류\n",
    "    if policy_model is not None and tfidf_vectorizer is not None:\n",
    "        # TF-IDF 벡터화 (이미 수행되었으면 재사용)\n",
    "        if 'text_tfidf' not in locals():\n",
    "            text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
    "        \n",
    "        # 정책 분야 예측\n",
    "        policy_area = policy_model.predict(text_tfidf)[0]\n",
    "    else:\n",
    "        # 규칙 기반 분류\n",
    "        policy_area = classify_policy_area(text, okt)\n",
    "    \n",
    "    results['policy_area'] = policy_area\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 샘플 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from types import SimpleNamespace\n",
    "    \n",
    "    # Jupyter Notebook 환경에서 실행될 때를 대비한 설정\n",
    "    try:\n",
    "        parser = argparse.ArgumentParser(description=\"대통령 후보 연설문 감성 분석 및 정책 분야 분류\")\n",
    "        parser.add_argument(\"--train\", action=\"store_true\", help=\"새 모델 학습 여부\")\n",
    "        parser.add_argument(\"--crawl\", action=\"store_true\", help=\"뉴스 데이터 크롤링 여부\")\n",
    "        parser.add_argument(\"--keywords\", nargs=\"+\", default=[\"대통령\", \"후보\", \"연설\"], help=\"크롤링 키워드\")\n",
    "        parser.add_argument(\"--pages\", type=int, default=3, help=\"크롤링할 페이지 수\")\n",
    "        args = parser.parse_args()\n",
    "    except:\n",
    "        # Jupyter Notebook 환경에서는 SimpleNamespace 사용\n",
    "        args = SimpleNamespace(\n",
    "            train=False,\n",
    "            crawl=False,\n",
    "            keywords=[\"대통령\", \"후보\", \"연설\"],\n",
    "            pages=3\n",
    "        )\n",
    "    \n",
    "    # 형태소 분석기 초기화\n",
    "    okt_instance = Okt()\n",
    "    print(\"OKT 형태소 분석기 초기화 완료\")\n",
    "    \n",
    "    # 감성 사전 로드\n",
    "    sentiment_dict = load_knu_sentiment_dict()\n",
    "    \n",
    "    # 크롤링 수행 (선택적)\n",
    "    if args.crawl:\n",
    "        # 안전한 경로 생성\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "        save_path = os.path.join(current_dir, \"data\", \"crawled_news.csv\")\n",
    "        \n",
    "        crawl_data = crawl_news_data(\n",
    "            keywords=args.keywords,\n",
    "            num_pages=args.pages,\n",
    "            save_path=save_path\n",
    "        )\n",
    "        print(f\"크롤링 결과: {len(crawl_data)}개 항목\")\n",
    "    \n",
    "    # 모델 학습 또는 로드\n",
    "    if args.train:\n",
    "        print(\"새 모델 학습 중...\")\n",
    "        sentiment_model, vectorizer, _, policy_model, _ = train_sentiment_model(\n",
    "            use_speech_data=True,\n",
    "            use_sentiword_data=True,\n",
    "            policy_area_classification=True,\n",
    "            save_model=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"저장된 모델 로드 중...\")\n",
    "        sentiment_model, vectorizer, policy_model = load_models(use_okt=True)\n",
    "        \n",
    "        # 모델 로드 실패 시 새 모델 학습\n",
    "        if sentiment_model is None or vectorizer is None:\n",
    "            print(\"저장된 모델을 찾을 수 없어 새 모델을 학습합니다.\")\n",
    "            sentiment_model, vectorizer, _, policy_model, _ = train_sentiment_model(\n",
    "                use_speech_data=True,\n",
    "                use_sentiword_data=True,\n",
    "                policy_area_classification=True,\n",
    "                save_model=True\n",
    "            )\n",
    "    \n",
    "    # 샘플 텍스트 감성 분석\n",
    "    sample_comments = [\n",
    "        \"경제 성장을 위한 규제 혁신 정책은 정말 기대됩니다!\",\n",
    "        \"이 후보의 복지 공약은 실현 가능성이 낮아 보입니다.\",\n",
    "        \"교육 정책이 구체적이지 않고 모호합니다.\",\n",
    "        \"환경 문제에 대한 강력한 대책, 매우 환영합니다.\",\n",
    "        \"안보와 국방 정책은 현실적이고 균형 잡힌 접근법이라고 생각합니다.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n샘플 텍스트 감성 분석 결과:\")\n",
    "    for comment in sample_comments:\n",
    "        results = analyze_text(comment, sentiment_model, vectorizer, okt_instance, policy_model, sentiment_dict)\n",
    "        print(f\"\\n댓글: '{comment}'\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        lexicon_result = results['lexicon_sentiment']\n",
    "        print(f\"→ 사전 감정: {lexicon_result['label']} (점수: {lexicon_result['score']:.4f})\")\n",
    "        \n",
    "        if 'model_sentiment' in results:\n",
    "            model_result = results['model_sentiment']\n",
    "            print(f\"→ 모델 감정: {model_result['label']} (확률: {model_result['confidence']:.4f})\")\n",
    "        \n",
    "        print(f\"→ 정책 분야: {results['policy_area']}\")\n",
    "    \n",
    "    # 대화형 모드 실행\n",
    "    print(\"\\n직접 텍스트를 입력하여 분석해보세요 (종료하려면 'exit' 입력):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\n분석할 텍스트 입력: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        results = analyze_text(user_input, sentiment_model, vectorizer, okt_instance, policy_model, sentiment_dict)\n",
    "        \n",
    "        # 결과 출력\n",
    "        lexicon_result = results['lexicon_sentiment']\n",
    "        print(f\"→ 사전 감정: {lexicon_result['label']} (점수: {lexicon_result['score']:.4f})\")\n",
    "        \n",
    "        if 'model_sentiment' in results:\n",
    "            model_result = results['model_sentiment']\n",
    "            print(f\"→ 모델 감정: {model_result['label']} (확률: {model_result['confidence']:.4f})\")\n",
    "        \n",
    "        print(f\"→ 정책 분야: {results['policy_area']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c51f8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train] [--crawl]\n",
      "                             [--keywords KEYWORDS [KEYWORDS ...]]\n",
      "                             [--pages PAGES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\woig9\\AppData\\Roaming\\jupyter\\runtime\\kernel-v39c3ef3451f401fcd5d0758316da778b16e692b66.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석기 초기화 완료\n",
      "KNU 감성 사전 로드 중...\n",
      "감성 사전 로드 완료: 14841개 단어\n",
      "저장된 모델 로드 중...\n",
      "저장된 모델을 찾을 수 없어 새 모델을 학습합니다.\n",
      "KNU 감성 사전 로드 중...\n",
      "감성 사전 로드 완료: 14841개 단어\n",
      "MeCab 초기화 오류: Install MeCab in order to use it: http://konlpy.org/en/latest/install/\n",
      "MeCab 설치 확인: https://konlpy.org/en/latest/install/\n",
      "\n",
      "샘플 텍스트 감성 분석 결과:\n",
      "\n",
      "댓글: '경제 성장을 위한 규제 혁신 정책은 정말 기대됩니다!'\n",
      "→ 사전 감정: 긍정 (점수: 1.5000)\n",
      "→ 정책 분야: 경제\n",
      "\n",
      "댓글: '이 후보의 복지 공약은 실현 가능성이 낮아 보입니다.'\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 복지\n",
      "\n",
      "댓글: '교육 정책이 구체적이지 않고 모호합니다.'\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 교육\n",
      "\n",
      "댓글: '환경 문제에 대한 강력한 대책, 매우 환영합니다.'\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 환경\n",
      "\n",
      "댓글: '안보와 국방 정책은 현실적이고 균형 잡힌 접근법이라고 생각합니다.'\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 안보\n",
      "\n",
      "직접 텍스트를 입력하여 분석해보세요 (종료하려면 'exit' 입력):\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 중립 (점수: 0.0000)\n",
      "→ 정책 분야: 기타\n",
      "→ 사전 감정: 긍정 (점수: 1.5000)\n",
      "→ 정책 분야: 경제\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SentiWord_info.json 데이터를 활용한 학습 데이터 생성\n",
    "def create_training_data_from_sentiword(sentiment_dict, sample_size=1000):\n",
    "    \"\"\"\n",
    "    KNU 감성 사전에서 학습 데이터 생성\n",
    "    \n",
    "    Parameters:\n",
    "    sentiment_dict (dict): 단어-감성 점수 딕셔너리\n",
    "    sample_size (int): 생성할 샘플 수\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 학습용 데이터프레임\n",
    "    \"\"\"\n",
    "    print(f\"감성 사전에서 학습 데이터 생성 중 (목표 샘플 수: {sample_size})...\")\n",
    "    \n",
    "    # 긍정/부정 단어 분류\n",
    "    positive_words = [word for word, score in sentiment_dict.items() if score > 0]\n",
    "    negative_words = [word for word, score in sentiment_dict.items() if score < 0]\n",
    "    \n",
    "    # 긍정/부정 단어 수 확인\n",
    "    print(f\"긍정 단어 수: {len(positive_words)}, 부정 단어 수: {len(negative_words)}\")\n",
    "    \n",
    "    # 각 범주별 목표 샘플 수 계산\n",
    "    target_per_category = sample_size // 2\n",
    "    \n",
    "    # 단어 샘플링 (범주별 목표 샘플 수 만큼, 중복 허용)\n",
    "    positive_samples = random.choices(positive_words, k=target_per_category)\n",
    "    negative_samples = random.choices(negative_words, k=target_per_category)\n",
    "    \n",
    "    # 학습 데이터 생성\n",
    "    training_data = []\n",
    "    \n",
    "    # 긍정 샘플\n",
    "    for word in positive_samples:\n",
    "        training_data.append({\n",
    "            'document': word,\n",
    "            'label': 1  # 긍정\n",
    "        })\n",
    "    \n",
    "    # 부정 샘플\n",
    "    for word in negative_samples:\n",
    "        training_data.append({\n",
    "            'document': word,\n",
    "            'label': 0  # 부정\n",
    "        })\n",
    "    \n",
    "    # 데이터프레임 생성 및 섞기\n",
    "    df = pd.DataFrame(training_data)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # 데이터 섞기\n",
    "    \n",
    "    print(f\"학습 데이터 생성 완료: {len(df)}개 샘플\")\n",
    "    return df\n",
    "\n",
    "# 웹 크롤링을 통한 뉴스 및 정치 관련 데이터 수집 함수\n",
    "def crawl_news_data(keywords, num_pages=1, save_path=None):\n",
    "    \"\"\"\n",
    "    requests와 BeautifulSoup을 사용한 뉴스 크롤링 함수\n",
    "    \n",
    "    Parameters:\n",
    "    keywords (list): 검색 키워드 리스트\n",
    "    num_pages (int): 수집할 페이지 수\n",
    "    save_path (str): 결과를 저장할 경로\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: 수집된 뉴스 데이터\n",
    "    \"\"\"\n",
    "    news_data = []\n",
    "    \n",
    "    print(f\"{', '.join(keywords)} 관련 뉴스 데이터 수집 중...\")\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        for page in range(1, num_pages + 1):\n",
    "            try:\n",
    "                # 네이버 뉴스 검색 URL (예시)\n",
    "                url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&start={(page-1)*10+1}\"\n",
    "                \n",
    "                # 요청 헤더 설정 (차단 방지)\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                }\n",
    "                \n",
    "                # 요청 보내기\n",
    "                response = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 뉴스 아이템 추출 (예시, 실제 선택자는 웹사이트에 따라 다름)\n",
    "                news_items = soup.select('.news_wrap')\n",
    "                \n",
    "                for item in news_items:\n",
    "                    try:\n",
    "                        # 제목과 요약 추출 (예시, 실제 선택자는 웹사이트에 따라 다름)\n",
    "                        title_elem = item.select_one('.news_tit')\n",
    "                        summary_elem = item.select_one('.dsc_wrap')\n",
    "                        \n",
    "                        if title_elem and summary_elem:\n",
    "                            title = title_elem.text.strip()\n",
    "                            summary = summary_elem.text.strip()\n",
    "                            \n",
    "                            news_data.append({\n",
    "                                'keyword': keyword,\n",
    "                                'title': title,\n",
    "                                'summary': summary,\n",
    "                                'content': f\"{title} {summary}\",\n",
    "                                'label': None  # 라벨은 나중에 수동으로 추가하거나 모델로 예측\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"뉴스 아이템 파싱 오류: {e}\")\n",
    "                \n",
    "                # 과도한 요청 방지를 위한 대기\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"페이지 크롤링 오류: {e}\")\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    \n",
    "    # 결과 저장\n",
    "    if save_path and len(news_data) > 0:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        news_df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "        print(f\"수집 데이터 저장 완료: {save_path}\")\n",
    "    \n",
    "    print(f\"뉴스 데이터 수집 완료: {len(news_data)}개 항목\")\n",
    "    return news_df\n",
    "\n",
    "# 메인 함수: 감성 사전 기반 및 ML 모델 결합\n",
    "def train_sentiment_model(use_speech_data=True, use_sentiword_data=True, policy_area_classification=True, save_model=True):\n",
    "    # KNU 감성 사전 로드\n",
    "    sentiment_dict = load_knu_sentiment_dict()\n",
    "    \n",
    "    # 형태소 분석기 초기화 - MeCab으로 변경\n",
    "    try:\n",
    "        mecab = Mecab()\n",
    "        print(\"MeCab 형태소 분석기 초기화 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"MeCab 초기화 오류: {e}\")\n",
    "        print(\"MeCab 설치 확인: https://konlpy.org/en/latest/install/\")\n",
    "        return None, None, None, None, sentiment_dict\n",
    "    \n",
    "    # 학습 데이터 준비\n",
    "    training_data = pd.DataFrame()\n",
    "    \n",
    "    # 1. 연설문 데이터 로드 (선택적)\n",
    "    if use_speech_data:\n",
    "        speech_data = load_or_create_speech_data()\n",
    "        print(f\"연설문 데이터 로드 완료 (샘플 수: {len(speech_data)}개)\")\n",
    "        training_data = pd.concat([training_data, speech_data])\n",
    "    \n",
    "    # 2. SentiWord_info.json 데이터에서 학습 데이터 생성 (선택적)\n",
    "    if use_sentiword_data and sentiment_dict:\n",
    "        sentiword_data = create_training_data_from_sentiword(sentiment_dict, sample_size=2000)\n",
    "        if not sentiword_data.empty:\n",
    "            # 'policy_area' 열이 없으면 추가\n",
    "            if 'policy_area' not in sentiword_data.columns:\n",
    "                sentiword_data['policy_area'] = 'none'\n",
    "            training_data = pd.concat([training_data, sentiword_data])\n",
    "    \n",
    "    if training_data.empty:\n",
    "        print(\"학습 데이터가 준비되지 않았습니다.\")\n",
    "        return None, None, None, None, sentiment_dict\n",
    "    \n",
    "    print(f\"총 학습 데이터 크기: {len(training_data)}개\")\n",
    "    \n",
    "    # 텍스트 전처리\n",
    "    training_data['processed_text'] = training_data['text'].apply(\n",
    "        lambda x: preprocess_text(x, mecab, is_political=True)\n",
    "    )\n",
    "    \n",
    "    # 특성 추출 (TF-IDF)\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        min_df=5,\n",
    "        max_df=0.7,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    X = tfidf_vectorizer.fit_transform(training_data['processed_text'])\n",
    "    y = training_data['sentiment']\n",
    "    \n",
    "    # 학습/테스트 데이터 분리\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 감성 분석 모델 학습 (LogisticRegression에서 RandomForest로 변경)\n",
    "    print(\"감성 분석 모델 학습 중...\")\n",
    "    sentiment_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sentiment_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 모델 평가\n",
    "    y_pred = sentiment_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"감성 분석 모델 정확도: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # 정책 분야 분류 모델 학습 (선택적)\n",
    "    policy_model = None\n",
    "    if policy_area_classification and 'policy_area' in training_data.columns:\n",
    "        policy_data = training_data[training_data['policy_area'] != 'none']\n",
    "        if len(policy_data) > 0:\n",
    "            print(\"정책 분야 분류 모델 학습 중...\")\n",
    "            X_policy = tfidf_vectorizer.transform(policy_data['processed_text'])\n",
    "            y_policy = policy_data['policy_area']\n",
    "            \n",
    "            # 학습/테스트 데이터 분리\n",
    "            X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(X_policy, y_policy, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # 정책 분야 분류 모델 (RandomForest 사용)\n",
    "            policy_model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            policy_model.fit(X_p_train, y_p_train)\n",
    "            \n",
    "            # 모델 평가\n",
    "            y_p_pred = policy_model.predict(X_p_test)\n",
    "            policy_accuracy = accuracy_score(y_p_test, y_p_pred)\n",
    "            print(f\"정책 분야 분류 모델 정확도: {policy_accuracy:.4f}\")\n",
    "            print(classification_report(y_p_test, y_p_pred))\n",
    "    \n",
    "    # 모델 저장 (선택적)\n",
    "    if save_model:\n",
    "        # Jupyter notebook에서는 __file__이 정의되지 않으므로 현재 작업 디렉토리 사용\n",
    "        model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # 감성 분석 모델 저장\n",
    "        with open(os.path.join(model_dir, \"sentiment_model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(sentiment_model, f)\n",
    "        \n",
    "        # TF-IDF 벡터라이저 저장\n",
    "        with open(os.path.join(model_dir, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(tfidf_vectorizer, f)\n",
    "        \n",
    "        # 정책 분야 분류 모델 저장 (있는 경우)\n",
    "        if policy_model is not None:\n",
    "            with open(os.path.join(model_dir, \"policy_model.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(policy_model, f)\n",
    "        \n",
    "        print(f\"모델 저장 완료: {model_dir}\")\n",
    "    \n",
    "    return sentiment_model, tfidf_vectorizer, mecab, policy_model, sentiment_dict\n",
    "\n",
    "def load_models(model_dir=None, use_okt=False):\n",
    "    \"\"\"\n",
    "    저장된 모델 파일 로드\n",
    "    \n",
    "    Parameters:\n",
    "    model_dir (str): 모델 파일이 저장된 디렉토리 경로\n",
    "    use_okt (bool): OKT 형태소 분석기 사용 여부 (기본값: False)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (sentiment_model, tfidf_vectorizer, policy_model) 튜플\n",
    "    \"\"\"\n",
    "    if model_dir is None:\n",
    "        # Jupyter notebook에서는 __file__이 정의되지 않으므로 현재 작업 디렉토리 사용\n",
    "        model_dir = os.path.join(os.getcwd(), \"models\")\n",
    "    \n",
    "    sentiment_model = None\n",
    "    tfidf_vectorizer = None\n",
    "    policy_model = None\n",
    "    \n",
    "    try:\n",
    "        # 감성 분석 모델 로드\n",
    "        sentiment_model_path = os.path.join(model_dir, \"sentiment_model.pkl\")\n",
    "        if os.path.exists(sentiment_model_path):\n",
    "            with open(sentiment_model_path, \"rb\") as f:\n",
    "                sentiment_model = pickle.load(f)\n",
    "            print(\"감성 분석 모델 로드 완료\")\n",
    "        \n",
    "        # TF-IDF 벡터라이저 로드\n",
    "        vectorizer_path = os.path.join(model_dir, \"tfidf_vectorizer.pkl\")\n",
    "        if os.path.exists(vectorizer_path):\n",
    "            with open(vectorizer_path, \"rb\") as f:\n",
    "                tfidf_vectorizer = pickle.load(f)\n",
    "            print(\"TF-IDF 벡터라이저 로드 완료\")\n",
    "        \n",
    "        # 정책 분야 분류 모델 로드\n",
    "        policy_model_path = os.path.join(model_dir, \"policy_model.pkl\")\n",
    "        if os.path.exists(policy_model_path):\n",
    "            with open(policy_model_path, \"rb\") as f:\n",
    "                policy_model = pickle.load(f)\n",
    "            print(\"정책 분야 분류 모델 로드 완료\")\n",
    "        \n",
    "        return sentiment_model, tfidf_vectorizer, policy_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 오류: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 새로운 텍스트에 대한 감성 분석 및 정책 분야 분류 함수\n",
    "def analyze_text(text, sentiment_model, tfidf_vectorizer, okt, policy_model, sentiment_dict):\n",
    "    \"\"\"\n",
    "    텍스트에 대한 종합 분석 수행: \n",
    "    1. 감성 사전 기반 감성 분석\n",
    "    2. ML 모델 기반 감성 분석 (모델이 있는 경우)\n",
    "    3. 정책 분야 분류 (모델이 있는 경우 모델 사용, 없으면 규칙 기반)\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): 분석할 텍스트\n",
    "    sentiment_model: 감성 분석 모델\n",
    "    tfidf_vectorizer: TF-IDF 벡터라이저\n",
    "    okt (Okt): OKT 형태소 분석기 객체\n",
    "    policy_model: 정책 분야 분류 모델\n",
    "    sentiment_dict (dict): 감성 사전\n",
    "    \n",
    "    Returns:\n",
    "    dict: 분석 결과\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 텍스트 전처리\n",
    "    processed_text = preprocess_text(text, okt, is_political=True)\n",
    "    \n",
    "    # 1. 감성 사전 기반 감성 분석\n",
    "    lexicon_sentiment, lexicon_score = analyze_sentiment_with_lexicon(text, okt, sentiment_dict)\n",
    "    results['lexicon_sentiment'] = {\n",
    "        'label': lexicon_sentiment,\n",
    "        'score': lexicon_score\n",
    "    }\n",
    "    \n",
    "    # 2. ML 모델 기반 감성 분석 (모델이 있는 경우)\n",
    "    if sentiment_model is not None and tfidf_vectorizer is not None:\n",
    "        # TF-IDF 벡터화\n",
    "        text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
    "        \n",
    "        # 감성 예측\n",
    "        sentiment = sentiment_model.predict(text_tfidf)[0]\n",
    "        probability = sentiment_model.predict_proba(text_tfidf)[0]\n",
    "        \n",
    "        sentiment_label = \"긍정\" if sentiment == 1 else \"부정\"\n",
    "        confidence = probability[1] if sentiment == 1 else probability[0]\n",
    "        \n",
    "        results['model_sentiment'] = {\n",
    "            'label': sentiment_label,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "    \n",
    "    # 3. 정책 분야 분류\n",
    "    if policy_model is not None and tfidf_vectorizer is not None:\n",
    "        # TF-IDF 벡터화 (이미 수행되었으면 재사용)\n",
    "        if 'text_tfidf' not in locals():\n",
    "            text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
    "        \n",
    "        # 정책 분야 예측\n",
    "        policy_area = policy_model.predict(text_tfidf)[0]\n",
    "    else:\n",
    "        # 규칙 기반 분류\n",
    "        policy_area = classify_policy_area(text, okt)\n",
    "    \n",
    "    results['policy_area'] = policy_area\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 샘플 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from types import SimpleNamespace\n",
    "    \n",
    "    # Jupyter Notebook 환경에서 실행될 때를 대비한 설정\n",
    "    try:\n",
    "        parser = argparse.ArgumentParser(description=\"대통령 후보 연설문 감성 분석 및 정책 분야 분류\")\n",
    "        parser.add_argument(\"--train\", action=\"store_true\", help=\"새 모델 학습 여부\")\n",
    "        parser.add_argument(\"--crawl\", action=\"store_true\", help=\"뉴스 데이터 크롤링 여부\")\n",
    "        parser.add_argument(\"--keywords\", nargs=\"+\", default=[\"대통령\", \"후보\", \"연설\"], help=\"크롤링 키워드\")\n",
    "        parser.add_argument(\"--pages\", type=int, default=3, help=\"크롤링할 페이지 수\")\n",
    "        args = parser.parse_args()\n",
    "    except:\n",
    "        # Jupyter Notebook 환경에서는 SimpleNamespace 사용\n",
    "        args = SimpleNamespace(\n",
    "            train=False,\n",
    "            crawl=False,\n",
    "            keywords=[\"대통령\", \"후보\", \"연설\"],\n",
    "            pages=3\n",
    "        )\n",
    "    \n",
    "    # 형태소 분석기 초기화\n",
    "    okt_instance = Okt()\n",
    "    print(\"OKT 형태소 분석기 초기화 완료\")\n",
    "    \n",
    "    # 감성 사전 로드\n",
    "    sentiment_dict = load_knu_sentiment_dict()\n",
    "    \n",
    "    # 크롤링 수행 (선택적)\n",
    "    if args.crawl:\n",
    "        # 안전한 경로 생성\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "        save_path = os.path.join(current_dir, \"data\", \"crawled_news.csv\")\n",
    "        \n",
    "        crawl_data = crawl_news_data(\n",
    "            keywords=args.keywords,\n",
    "            num_pages=args.pages,\n",
    "            save_path=save_path\n",
    "        )\n",
    "        print(f\"크롤링 결과: {len(crawl_data)}개 항목\")\n",
    "    \n",
    "    # 모델 학습 또는 로드\n",
    "    if args.train:\n",
    "        print(\"새 모델 학습 중...\")\n",
    "        sentiment_model, vectorizer, _, policy_model, _ = train_sentiment_model(\n",
    "            use_speech_data=True,\n",
    "            use_sentiword_data=True,\n",
    "            policy_area_classification=True,\n",
    "            save_model=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"저장된 모델 로드 중...\")\n",
    "        sentiment_model, vectorizer, policy_model = load_models(use_okt=True)\n",
    "        \n",
    "        # 모델 로드 실패 시 새 모델 학습\n",
    "        if sentiment_model is None or vectorizer is None:\n",
    "            print(\"저장된 모델을 찾을 수 없어 새 모델을 학습합니다.\")\n",
    "            sentiment_model, vectorizer, _, policy_model, _ = train_sentiment_model(\n",
    "                use_speech_data=True,\n",
    "                use_sentiword_data=True,\n",
    "                policy_area_classification=True,\n",
    "                save_model=True\n",
    "            )\n",
    "    \n",
    "    # 샘플 텍스트 감성 분석\n",
    "    sample_comments = [\n",
    "        \"경제 성장을 위한 규제 혁신 정책은 정말 기대됩니다!\",\n",
    "        \"이 후보의 복지 공약은 실현 가능성이 낮아 보입니다.\",\n",
    "        \"교육 정책이 구체적이지 않고 모호합니다.\",\n",
    "        \"환경 문제에 대한 강력한 대책, 매우 환영합니다.\",\n",
    "        \"안보와 국방 정책은 현실적이고 균형 잡힌 접근법이라고 생각합니다.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n샘플 텍스트 감성 분석 결과:\")\n",
    "    for comment in sample_comments:\n",
    "        results = analyze_text(comment, sentiment_model, vectorizer, okt_instance, policy_model, sentiment_dict)\n",
    "        print(f\"\\n댓글: '{comment}'\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        lexicon_result = results['lexicon_sentiment']\n",
    "        print(f\"→ 사전 감정: {lexicon_result['label']} (점수: {lexicon_result['score']:.4f})\")\n",
    "        \n",
    "        if 'model_sentiment' in results:\n",
    "            model_result = results['model_sentiment']\n",
    "            print(f\"→ 모델 감정: {model_result['label']} (확률: {model_result['confidence']:.4f})\")\n",
    "        \n",
    "        print(f\"→ 정책 분야: {results['policy_area']}\")\n",
    "    \n",
    "    # 대화형 모드 실행\n",
    "    print(\"\\n직접 텍스트를 입력하여 분석해보세요 (종료하려면 'exit' 입력):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\n분석할 텍스트 입력: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        results = analyze_text(user_input, sentiment_model, vectorizer, okt_instance, policy_model, sentiment_dict)\n",
    "        \n",
    "        # 결과 출력\n",
    "        lexicon_result = results['lexicon_sentiment']\n",
    "        print(f\"→ 사전 감정: {lexicon_result['label']} (점수: {lexicon_result['score']:.4f})\")\n",
    "        \n",
    "        if 'model_sentiment' in results:\n",
    "            model_result = results['model_sentiment']\n",
    "            print(f\"→ 모델 감정: {model_result['label']} (확률: {model_result['confidence']:.4f})\")\n",
    "        \n",
    "        print(f\"→ 정책 분야: {results['policy_area']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
